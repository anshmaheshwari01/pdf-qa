# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ek_gPhyIdGTRgs9Mwm8qWpXrE_qfzQTd
"""

import streamlit as st
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from transformers import pipeline

# Load and prepare data only once using Streamlit cache
@st.cache_resource
def setup_qa():
    loader = PyMuPDFLoader("Nonfiction Reading Test Black Friday.pdf")
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(documents)

    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma.from_documents(chunks, embedding=embedding)
    retriever = db.as_retriever()

    qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")
    return retriever, qa_pipeline

retriever, qa_pipeline = setup_qa()

# Streamlit UI
st.set_page_config(page_title="üìò PDF Q&A Chatbot", layout="centered")
st.title("üìò Ask Questions from PDF")
st.markdown("Upload: *Nonfiction Reading Test Black Friday.pdf*")

user_input = st.text_input("Enter your question:")

if user_input:
    docs = retriever.get_relevant_documents(user_input)
    context = "\n\n".join(doc.page_content for doc in docs[:3])

    if not context.strip():
        st.warning("‚ö†Ô∏è No relevant content found.")
    else:
        result = qa_pipeline({
            "question": user_input,
            "context": context
        })
        st.success(f"**Answer:** {result['answer']}")